{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44317019-92f6-4835-999d-321b67eb0c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Create a noisy dataset with 2 classes and 3 informative features\n",
    "X, y = make_classification(\n",
    "    n_samples=500,    # 500 samples\n",
    "    n_features=4,     # 4 features\n",
    "    n_classes=2,      # 2 classes\n",
    "    n_informative=3,  # 3 informative features\n",
    "    n_redundant=1,    # 1 redundant feature\n",
    "    random_state=42,  # Set seed for reproducibility\n",
    "    flip_y=0.1,       # Introduce some noise by flipping 10% of labels\n",
    ")\n",
    "\n",
    "# Step 2: Split into training and testing datasets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Initialize the model (Logistic Regression)\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Step 4: Variables to store results\n",
    "batch_accuracies = []\n",
    "sample_sizes = []\n",
    "improvement = []\n",
    "\n",
    "# Step 5: Train the model on progressively larger batches of data\n",
    "batch_size = len(X_train) // 5  # Split training set into 5 parts\n",
    "\n",
    "for i in range(1, 6):\n",
    "    # Training on the first i batches of data\n",
    "    X_train_batch = X_train[:i * batch_size]\n",
    "    y_train_batch = y_train[:i * batch_size]\n",
    "    \n",
    "    model.fit(X_train_batch, y_train_batch)  # Train the model\n",
    "    \n",
    "    # Predict on the test set and calculate accuracy\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    batch_accuracies.append(acc)\n",
    "    sample_sizes.append(i * batch_size)\n",
    "\n",
    "    # Track accuracy improvement\n",
    "    if i > 1:\n",
    "        improvement.append(batch_accuracies[i-1] - batch_accuracies[i-2])\n",
    "    else:\n",
    "        improvement.append('N/A')  # No improvement for the first batch\n",
    "\n",
    "# Step 6: Calculate Adaptation Efficiency Score (AES)\n",
    "AES = []\n",
    "for i in range(1, len(batch_accuracies)):\n",
    "    if improvement[i] != 'N/A':\n",
    "        improvement_rate = improvement[i] / max(sample_sizes[i], 1)  # Avoid divide by zero\n",
    "        AES.append(improvement_rate)  # Adjust improvement per sample seen\n",
    "    else:\n",
    "        AES.append('N/A')\n",
    "\n",
    "# Step 7: Ensure that all lists have the same length for DataFrame\n",
    "AES = ['N/A'] + AES  # Add 'N/A' for the first batch\n",
    "\n",
    "# Step 8: Calculate the Final Accuracy (Traditional Metric)\n",
    "final_accuracy = batch_accuracies[-1]\n",
    "\n",
    "# Show the results in a clean format\n",
    "results_df = pd.DataFrame({\n",
    "    'Batch': range(1, 6),\n",
    "    'Accuracy': batch_accuracies,\n",
    "    'Improvement': improvement,\n",
    "    'Samples Seen': sample_sizes,\n",
    "    'AES': AES\n",
    "})\n",
    "\n",
    "# Final output of the model's performance\n",
    "print(f\"Adaptation Efficiency Score (AES): {np.mean([a for a in AES if a != 'N/A']):.4f}\")\n",
    "print(f\"Final Test Accuracy (Traditional Metric): {final_accuracy:.4f}\")\n",
    "\n",
    "# Display the result as a table\n",
    "print(\"\\nAccuracy, Improvement, and AES per Batch:\")\n",
    "print(results_df)\n",
    "\n",
    "# Plot the learning curve for better understanding\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot Accuracy vs Samples Seen\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, 6), batch_accuracies, marker='o', color='b', label=\"Accuracy\")\n",
    "plt.xlabel('Batch number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model Performance - Accuracy')\n",
    "plt.xticks(range(1, 6))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Plot AES vs Samples Seen\n",
    "plt.subplot(1, 2, 2)\n",
    "# Convert AES to numeric and ignore 'N/A'\n",
    "AES_numeric = [a if a != 'N/A' else np.nan for a in AES]\n",
    "plt.plot(range(1, 6), AES_numeric, marker='o', color='r', label=\"AES\")\n",
    "plt.xlabel('Batch number')\n",
    "plt.ylabel('Adaptation Efficiency Score (AES)')\n",
    "plt.title('Model Adaptation Efficiency')\n",
    "plt.xticks(range(1, 6))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Show both plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Interpretation of AES\n",
    "if np.mean([a for a in AES if a != 'N/A']) > 0.02:\n",
    "    print(\"The model shows efficient adaptation to new data.\")\n",
    "else:\n",
    "    print(\"The model shows slow adaptation to new data.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
